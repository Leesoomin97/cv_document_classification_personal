{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb28a51a-38aa-4bae-9a4f-661fcd06983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking OCR extraction quality...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking train_texts_v6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1570/1570 [00:01<00:00, 886.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Folder: /data/ephemeral/home/data/ocr/train_texts_v6\n",
      " - Total expected: 1570\n",
      " - Found: 1570 (100.00%)\n",
      " - Missing: 0 (0.00%)\n",
      " - Empty: 0 (0.00%)\n",
      " - Avg text length: 343.1\n",
      " - Min/Max length: 2 / 1308\n",
      "\n",
      "ü™∂ Sample texts:\n",
      " - 002f99746285dfdd.jpg: 72Ïò§\n",
      " - 008ccd231e1fea5d.jpg: ÏßÑÎ£åÎπÑ(ÏïΩÏ†úÎπÑ) ÎÇ±ÏûÖ ÌôïÏù∏ÏÑú :ÏûêÎèôÎ¨¥Î≤àÌò∏ Ï£ºÏù∏ÌïòÎ¨¥Î©¥Ìò∏ Í¥Ä   Ïûê ÏòÅ E Ìè¨ÎØ∏Î≤ÑÎã§ Ïã†CUlleÎ¨¥Î∞∞) UÎ†à Ïä§ÌÉÄaÌÑ∞ Oe 8CJ ÎπÑÍ∑ºÏó¨ Î¨¥Íæ∏e (u+J Ïñ¥Ïù¥ÎÇ¥Îã§ ÎÇ¥C (U6>U_ 8e8 6646 ÏïÑ0Î¶¨ Ïöîa @Ìã∞yJ 70-2 ÏöîÌûà euG5 70 Ïöîa @Î∂Ä)) OSes - ...\n",
      " - 008f5911bfda7695.jpg: ÏßÑÎ£åÎπÑ(ÏïΩÏ†úÎπÑ) }ÏûÖ ÌôïÏù∏ÏÑú ÌïúÏûê ÏÑ¨ Î™Ö Ï£ºÎü∞Îì± ÎÇòÏûëÌò∏ ÏßÑ_ÎπÑ(ÏÑ†-Îãà) ÎÇ¥Ïô∏ ÏÜåÍ∑πÎ™πÍ∏∞ ÎåÄÏÉÅÎ†• ÎπÑÌÉÄÏù¥ Ï†Ñ: ; Ï£º Î¶¨ :P Î≥∏ÏÑ†Î∂ÄÎã§ (kD Í∞ÄÎìú Ïù¥Î¶¨) @3 ] (CSa) r8 3#ÏòÅ J4) 'o Jx 39430 39,103 (l V 47 v4o 4O VU ÏÜåÎìùÍ∑úÍ±∞ ÎçîÏÇ¨...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking test_texts_v6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3140/3140 [00:06<00:00, 483.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Folder: /data/ephemeral/home/data/ocr/test_texts_v6\n",
      " - Total expected: 3140\n",
      " - Found: 3140 (100.00%)\n",
      " - Missing: 0 (0.00%)\n",
      " - Empty: 192 (6.11%)\n",
      " - Avg text length: 101.6\n",
      " - Min/Max length: 0 / 1255\n",
      "\n",
      "ü™∂ Sample texts:\n",
      " - 0008fdb22ddce0ce.jpg: Uoa OaDI ÌÜ†Ìä∏ Ìëπ %\n",
      " - 00091bffdffd83de.jpg: \n",
      " - 00396fbc1f6cc21d.jpg: KO 'IU I Ïò¨ R0 5 i 5 Î∂ô Ïö© Ï§ë Î¨º 8 # Ïû•0 NIO ÏûêjE ÌçΩ 8 Ïö∏ Í≤® 16 a(ÏÑú Ïö© Íµ≠ ÌÜµ 1 Î≤Ñ Í∫º ÏÑúO C\n",
      "\n",
      "‚úÖ OCR check finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# check_ocr_text_quality.py\n",
    "# OCR ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú ÌíàÏßà Ï†êÍ≤Ä\n",
    "#  - ÎàÑÎùΩ ÌååÏùº, Îπà ÌÖçÏä§Ìä∏ ÎπÑÏú®, ÌèâÍ∑† Í∏∏Ïù¥, ÏÉòÌîå ÎÇ¥Ïö© ÌôïÏù∏\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE = \"/data/ephemeral/home/data\"\n",
    "OCR_TRAIN_DIR = f\"{BASE}/ocr/train_texts_v6\"\n",
    "OCR_TEST_DIR  = f\"{BASE}/ocr/test_texts_v6\"\n",
    "TRAIN_META = f\"{BASE}/meta_stage0_6_train_v6.csv\"\n",
    "TRAIN_CSV  = f\"{BASE}/raw/train.csv\"\n",
    "SUB_CSV    = f\"{BASE}/raw/sample_submission.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# Helper\n",
    "# ----------------------------\n",
    "def load_text(path):\n",
    "    if not os.path.exists(path):\n",
    "        return \"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        txt = f.read().strip()\n",
    "    return txt\n",
    "\n",
    "def analyze_folder(ocr_dir, expected_ids):\n",
    "    stats = {\"found\": 0, \"missing\": 0, \"empty\": 0, \"lengths\": []}\n",
    "    missing_files = []\n",
    "\n",
    "    for id_ in tqdm(expected_ids, desc=f\"Checking {os.path.basename(ocr_dir)}\"):\n",
    "        stem = os.path.splitext(id_)[0]\n",
    "        candidates = glob.glob(os.path.join(ocr_dir, f\"{stem}.*\"))\n",
    "        if not candidates:\n",
    "            stats[\"missing\"] += 1\n",
    "            missing_files.append(id_)\n",
    "            continue\n",
    "\n",
    "        txt = load_text(candidates[0])\n",
    "        if len(txt.strip()) == 0:\n",
    "            stats[\"empty\"] += 1\n",
    "        stats[\"found\"] += 1\n",
    "        stats[\"lengths\"].append(len(txt))\n",
    "\n",
    "    total = len(expected_ids)\n",
    "    avg_len = sum(stats[\"lengths\"]) / max(1, len(stats[\"lengths\"]))\n",
    "    print(f\"\\nüìÇ Folder: {ocr_dir}\")\n",
    "    print(f\" - Total expected: {total}\")\n",
    "    print(f\" - Found: {stats['found']} ({stats['found']/total:.2%})\")\n",
    "    print(f\" - Missing: {stats['missing']} ({stats['missing']/total:.2%})\")\n",
    "    print(f\" - Empty: {stats['empty']} ({stats['empty']/total:.2%})\")\n",
    "    print(f\" - Avg text length: {avg_len:.1f}\")\n",
    "    print(f\" - Min/Max length: {min(stats['lengths'] or [0])} / {max(stats['lengths'] or [0])}\")\n",
    "\n",
    "    # ÏÉòÌîå 3Í∞ú Ï∂úÎ†•\n",
    "    print(\"\\nü™∂ Sample texts:\")\n",
    "    sample_ids = expected_ids[:3]\n",
    "    for sid in sample_ids:\n",
    "        cands = glob.glob(os.path.join(ocr_dir, f\"{os.path.splitext(sid)[0]}.*\"))\n",
    "        if not cands: continue\n",
    "        with open(cands[0], \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            txt = f.read().strip().replace(\"\\n\", \" \")\n",
    "        print(f\" - {sid}: {txt[:150]}{'...' if len(txt) > 150 else ''}\")\n",
    "\n",
    "    if stats[\"missing\"] > 0:\n",
    "        print(\"\\n‚ö†Ô∏è Missing file examples:\")\n",
    "        print(stats[\"missing\"], \"missing files, showing first 5 ‚Üí\", missing_files[:5])\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ----------------------------\n",
    "# Train / Test ÌôïÏù∏\n",
    "# ----------------------------\n",
    "train_meta = pd.read_csv(TRAIN_META)\n",
    "train_csv  = pd.read_csv(TRAIN_CSV)\n",
    "train_csv[\"basename\"] = train_csv[\"ID\"].apply(lambda x: f\"{x}.jpg\" if not str(x).endswith(\".jpg\") else x)\n",
    "\n",
    "# train set Í∏∞Ï§Ä (meta Í∏∞Ï§ÄÏúºÎ°úÎèÑ Í∞ÄÎä•)\n",
    "train_ids = train_csv[\"ID\"].tolist()\n",
    "test_ids  = pd.read_csv(SUB_CSV)[\"ID\"].tolist()\n",
    "\n",
    "print(\"üîç Checking OCR extraction quality...\\n\")\n",
    "train_stats = analyze_folder(OCR_TRAIN_DIR, train_ids)\n",
    "test_stats  = analyze_folder(OCR_TEST_DIR, test_ids)\n",
    "\n",
    "print(\"\\n‚úÖ OCR check finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b754c9b6-b12d-4d4f-9357-50b333caaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ocr_prob_generator_train_v6.py\n",
    "# OCR ÌÖçÏä§Ìä∏ Í∏∞Î∞ò Î¨∏ÏÑú Î∂ÑÎ•ò ÌôïÎ•† ÏÉùÏÑ± (RoBERTa fine-tuning, ÏïàÏ†ï ÏÑ∏ÌåÖ)\n",
    "#  - Train 5-Fold Stratified + EarlyStopping + Cosine Scheduler + fp16\n",
    "#  - Outputs:\n",
    "#       /interim/ocr_valid_probs/fold{fold}_ocr_valid.csv\n",
    "#       /interim/ocr_test_probs.csv\n",
    "#       ./ocr_model_folds/fold{fold}/best/ (Í∞Å Ìè¥Îìú Î≤†Ïä§Ìä∏ Î™®Îç∏)\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, re, random, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "BASE = \"/data/ephemeral/home/data\"\n",
    "OCR_TEXT_DIR_TRAIN = f\"{BASE}/ocr/train_texts_v6\"   # *.txt / *.json (id Í∏∞Î∞ò ÌååÏùºÎ™Ö)\n",
    "OCR_TEXT_DIR_TEST  = f\"{BASE}/ocr/test_texts_v6\"\n",
    "TRAIN_META  = f\"{BASE}/meta_stage0_6_train_v6.csv\"\n",
    "TRAIN_CSV   = f\"{BASE}/raw/train.csv\"\n",
    "SUB_CSV     = f\"{BASE}/raw/sample_submission.csv\"\n",
    "\n",
    "SAVE_VALID_DIR = f\"{BASE}/interim/ocr_valid_probs\"\n",
    "SAVE_TEST_PATH = f\"{BASE}/interim/ocr_test_probs.csv\"\n",
    "os.makedirs(SAVE_VALID_DIR, exist_ok=True)\n",
    "os.makedirs(\"./ocr_model_folds\", exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-base\"\n",
    "NUM_CLASSES = 17\n",
    "NFOLDS = 5\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# OCR Ï†ÑÏö© ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ (ÌÖçÏä§Ìä∏ ÌååÏù∏ÌäúÎãù ÏïàÏ†ï ÏÑ∏ÌåÖ)\n",
    "LR = 5e-5              # Ïù¥ÎØ∏ÏßÄ Î∞±Î≥∏(1e-4)Í≥º Îã¨Î¶¨ ÌÖçÏä§Ìä∏Îäî 5e-5Í∞Ä ÏïàÏ†Ñ\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "WARMUP_RATIO = 0.1\n",
    "PATIENCE = 5           # ÏñºÎ¶¨Ïä§ÌÉë Ïù∏ÎÇ¥\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa85cc7a-980a-47b4-92cf-b95d0e7b6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Reproducibility\n",
    "# ============================================================\n",
    "def seed_everything(seed=SEED):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Utils\n",
    "# ============================================================\n",
    "def load_ocr_text(filepath):\n",
    "    \"\"\"OCR ÌÖçÏä§Ìä∏ ÌååÏùº(txt/json) Î°úÎìú\"\"\"\n",
    "    text = \"\"\n",
    "    if filepath.endswith(\".txt\"):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    elif filepath.endswith(\".json\"):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            js = json.load(f)\n",
    "            if isinstance(js, dict) and \"text\" in js:\n",
    "                text = js[\"text\"]\n",
    "            elif isinstance(js, list):\n",
    "                # [{ \"text\": \"...\"} ...] ÌòïÌÉú\n",
    "                text = \" \".join([x.get(\"text\", \"\") for x in js if isinstance(x, dict)])\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text[:3000]\n",
    "\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.texts[idx] if self.texts[idx] else \"[EMPTY]\"\n",
    "        enc = self.tokenizer(\n",
    "            txt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_probs(model, dataset, batch_size=BATCH_SIZE):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items() if k != \"labels\"}\n",
    "        out = model(**batch)\n",
    "        probs = torch.softmax(out.logits, dim=1).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61da9f97-85ea-4c97-b2b6-192bdfb4f60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load OCR train texts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1570/1570 [00:01<00:00, 894.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Train OCR empty ratio: 0.000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load train meta and join targets\n",
    "# ============================================================\n",
    "meta = pd.read_csv(TRAIN_META)\n",
    "train_csv = pd.read_csv(TRAIN_CSV)\n",
    "train_csv[\"basename\"] = train_csv[\"ID\"].apply(lambda x: f\"{x}.jpg\" if not str(x).endswith(\".jpg\") else x)\n",
    "meta[\"basename\"] = meta[\"filepath\"].apply(os.path.basename)\n",
    "\n",
    "df = pd.merge(meta, train_csv[[\"basename\", \"target\"]], on=\"basename\", how=\"left\").dropna(subset=[\"target\"])\n",
    "df[\"target\"] = df[\"target\"].astype(int)\n",
    "\n",
    "# OCR ÌÖçÏä§Ìä∏ Î°úÎìú\n",
    "texts = []\n",
    "for b in tqdm(df[\"basename\"], desc=\"Load OCR train texts\"):\n",
    "    stem = os.path.splitext(b)[0]\n",
    "    cands = glob.glob(os.path.join(OCR_TEXT_DIR_TRAIN, f\"{stem}.*\"))\n",
    "    texts.append(load_ocr_text(cands[0]) if cands else \"\")\n",
    "df[\"ocr_text\"] = texts\n",
    "\n",
    "empty_ratio = (df[\"ocr_text\"].str.len() == 0).mean()\n",
    "print(f\"[Info] Train OCR empty ratio: {empty_ratio:.3f} ({empty_ratio*100:.1f}%)\")\n",
    "if empty_ratio > 0.9:\n",
    "    print(\"[Warn] Í±∞Ïùò Î™®Îì† OCR ÌÖçÏä§Ìä∏Í∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§. /ocr/train_texts_v6 ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ï†êÍ≤ÄÌïòÏÑ∏Ïöî.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0159b135-4cfa-4403-85b0-bbeb302508f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_51725/999916646.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='711' max='2370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 711/2370 02:17 < 05:22, 5.14 it/s, Epoch 9/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.824290</td>\n",
       "      <td>0.582803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.394000</td>\n",
       "      <td>0.759756</td>\n",
       "      <td>0.831210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.954700</td>\n",
       "      <td>0.380793</td>\n",
       "      <td>0.926752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.184982</td>\n",
       "      <td>0.964968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.225895</td>\n",
       "      <td>0.958599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.089200</td>\n",
       "      <td>0.187781</td>\n",
       "      <td>0.964968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.252720</td>\n",
       "      <td>0.961783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.247413</td>\n",
       "      <td>0.958599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.202010</td>\n",
       "      <td>0.971338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved valid probs: /data/ephemeral/home/data/interim/ocr_valid_probs/fold0_ocr_valid.csv\n",
      "\n",
      "===== Fold 2/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_51725/999916646.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='711' max='2370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 711/2370 02:20 < 05:29, 5.04 it/s, Epoch 9/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.864179</td>\n",
       "      <td>0.573248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.392100</td>\n",
       "      <td>0.914994</td>\n",
       "      <td>0.840764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.084100</td>\n",
       "      <td>0.219467</td>\n",
       "      <td>0.977707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.089285</td>\n",
       "      <td>0.984076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.102528</td>\n",
       "      <td>0.980892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.166799</td>\n",
       "      <td>0.974522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.184608</td>\n",
       "      <td>0.968153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.094590</td>\n",
       "      <td>0.984076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.190982</td>\n",
       "      <td>0.971338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved valid probs: /data/ephemeral/home/data/interim/ocr_valid_probs/fold1_ocr_valid.csv\n",
      "\n",
      "===== Fold 3/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_51725/999916646.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1185' max='2370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1185/2370 03:54 < 03:54, 5.06 it/s, Epoch 15/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.876112</td>\n",
       "      <td>0.582803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.415200</td>\n",
       "      <td>0.820304</td>\n",
       "      <td>0.828025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.019400</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.942675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.132128</td>\n",
       "      <td>0.968153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.117007</td>\n",
       "      <td>0.977707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>0.152713</td>\n",
       "      <td>0.968153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.099823</td>\n",
       "      <td>0.984076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.143323</td>\n",
       "      <td>0.977707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.133823</td>\n",
       "      <td>0.974522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.088025</td>\n",
       "      <td>0.977707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.136544</td>\n",
       "      <td>0.974522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.119677</td>\n",
       "      <td>0.984076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.132173</td>\n",
       "      <td>0.977707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.127376</td>\n",
       "      <td>0.980892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.980892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved valid probs: /data/ephemeral/home/data/interim/ocr_valid_probs/fold2_ocr_valid.csv\n",
      "\n",
      "===== Fold 4/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_51725/999916646.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='711' max='2370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 711/2370 02:20 < 05:29, 5.04 it/s, Epoch 9/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.922950</td>\n",
       "      <td>0.484076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.415700</td>\n",
       "      <td>1.096885</td>\n",
       "      <td>0.691083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.148800</td>\n",
       "      <td>0.438108</td>\n",
       "      <td>0.917197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.172654</td>\n",
       "      <td>0.968153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.212659</td>\n",
       "      <td>0.958599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.231031</td>\n",
       "      <td>0.958599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.186444</td>\n",
       "      <td>0.968153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.189450</td>\n",
       "      <td>0.971338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.178226</td>\n",
       "      <td>0.974522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved valid probs: /data/ephemeral/home/data/interim/ocr_valid_probs/fold3_ocr_valid.csv\n",
      "\n",
      "===== Fold 5/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_51725/999916646.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1106' max='2370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1106/2370 03:37 < 04:08, 5.08 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.870315</td>\n",
       "      <td>0.576433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.412500</td>\n",
       "      <td>0.861485</td>\n",
       "      <td>0.780255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.990700</td>\n",
       "      <td>0.407329</td>\n",
       "      <td>0.901274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.497470</td>\n",
       "      <td>0.885350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.526364</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.428583</td>\n",
       "      <td>0.929936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.359364</td>\n",
       "      <td>0.939490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.463556</td>\n",
       "      <td>0.933121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.289539</td>\n",
       "      <td>0.949045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.389464</td>\n",
       "      <td>0.939490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.409267</td>\n",
       "      <td>0.939490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>0.324852</td>\n",
       "      <td>0.952229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.357444</td>\n",
       "      <td>0.952229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.382816</td>\n",
       "      <td>0.949045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved valid probs: /data/ephemeral/home/data/interim/ocr_valid_probs/fold4_ocr_valid.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Tokenizer\n",
    "# ============================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5-Fold Training\n",
    "# ============================================================\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(df, df[\"target\"])):\n",
    "    print(f\"\\n===== Fold {fold+1}/{NFOLDS} =====\")\n",
    "    tr_df, val_df = df.iloc[tr_idx].reset_index(drop=True), df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    tr_ds  = OCRDataset(tr_df[\"ocr_text\"].tolist(), tr_df[\"target\"].tolist(), tokenizer)\n",
    "    val_ds = OCRDataset(val_df[\"ocr_text\"].tolist(), val_df[\"target\"].tolist(), tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./ocr_model_folds/fold{fold}\",\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=100,\n",
    "        fp16=(DEVICE == \"cuda\"),\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        acc = (preds == labels).mean()\n",
    "        return {\"accuracy\": acc}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tr_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # --- Î≤†Ïä§Ìä∏ Í∞ÄÏ§ëÏπò Ï†ÄÏû• ---\n",
    "    best_dir = f\"./ocr_model_folds/fold{fold}/best\"\n",
    "    os.makedirs(best_dir, exist_ok=True)\n",
    "\n",
    "    model.config.id2label = {i: str(i) for i in range(NUM_CLASSES)}\n",
    "    model.config.label2id = {str(i): i for i in range(NUM_CLASSES)}\n",
    "\n",
    "    trainer.save_model(best_dir)\n",
    "\n",
    "    # --- Validation probs Ï†ÄÏû• ---\n",
    "    best_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        best_dir, num_labels=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "    val_probs = infer_probs(best_model, val_ds, batch_size=BATCH_SIZE)\n",
    "    out_df = pd.DataFrame(val_probs, columns=[f\"prob_{i}\" for i in range(NUM_CLASSES)])\n",
    "    out_df.insert(0, \"basename\", val_df[\"basename\"])\n",
    "    out_path = os.path.join(SAVE_VALID_DIR, f\"fold{fold}_ocr_valid.csv\")\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    print(f\"‚úÖ Saved valid probs: {out_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fbb30f-64ac-419e-9155-559f36cd32b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load OCR test texts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3140/3140 [00:06<00:00, 472.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test OCR empty ratio: 0.061 (6.1%)\n",
      "‚úÖ Saved test OCR probs: /data/ephemeral/home/data/interim/ocr_test_probs.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Test probs (fold-average)\n",
    "# ============================================================\n",
    "sub = pd.read_csv(SUB_CSV)\n",
    "test_texts = []\n",
    "for id_ in tqdm(sub[\"ID\"], desc=\"Load OCR test texts\"):\n",
    "    stem = os.path.splitext(id_)[0]\n",
    "    cands = glob.glob(os.path.join(OCR_TEXT_DIR_TEST, f\"{stem}.*\"))\n",
    "    test_texts.append(load_ocr_text(cands[0]) if cands else \"\")\n",
    "\n",
    "empty_ratio_t = (pd.Series(test_texts).str.len() == 0).mean()\n",
    "print(f\"[Info] Test OCR empty ratio: {empty_ratio_t:.3f} ({empty_ratio_t*100:.1f}%)\")\n",
    "if empty_ratio_t > 0.9:\n",
    "    print(\"[Warn] ÌÖåÏä§Ìä∏ OCR ÌÖçÏä§Ìä∏Í∞Ä Í±∞Ïùò ÎπÑÏñ¥ÏûàÏäµÎãàÎã§. /ocr/test_texts_v6 ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ï†êÍ≤ÄÌïòÏÑ∏Ïöî.\")\n",
    "\n",
    "test_ds = OCRDataset(test_texts, labels=None, tokenizer=tokenizer)\n",
    "test_probs_all = []\n",
    "\n",
    "for fold in range(NFOLDS):\n",
    "    best_dir = f\"./ocr_model_folds/fold{fold}/best\"\n",
    "    if not os.path.exists(best_dir):\n",
    "        best_dir = f\"./ocr_model_folds/fold{fold}\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        best_dir, num_labels=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "    probs = infer_probs(model, test_ds, batch_size=BATCH_SIZE)\n",
    "    test_probs_all.append(probs)\n",
    "\n",
    "final_test_probs = np.mean(test_probs_all, axis=0)\n",
    "out_df = pd.DataFrame(final_test_probs, columns=[f\"prob_{i}\" for i in range(NUM_CLASSES)])\n",
    "out_df.insert(0, \"ID\", sub[\"ID\"])\n",
    "out_df.to_csv(SAVE_TEST_PATH, index=False)\n",
    "print(f\"‚úÖ Saved test OCR probs: {SAVE_TEST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492caae5-e728-41f4-8df3-902b9e790cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
