{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75fe4c0-496e-409b-8882-9a631ed2013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# v7_holdout_ensemble_b3_vit_convnext.py\n",
    "# ------------------------------------------------------------\n",
    "# - Train data: v7 (offline augmented)\n",
    "#     BASE/meta_stage0_7_train_v7.csv (filepath, group, basename)\n",
    "# - Raw labels: BASE/raw/train.csv (ID, target)\n",
    "# - Test data: v6\n",
    "#     BASE/processed/stage0_6_test_v6/, BASE/raw/sample_submission.csv\n",
    "# - Strategy:\n",
    "#     1) Holdout 8:2 (by original ID, stratified)\n",
    "#     2) Train 3 models: EffB3, ViT-B16, ConvNeXt-S\n",
    "#     3) Use all aug of train IDs for training\n",
    "#     4) Holdout evalÏùÄ ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎßå ÏÇ¨Ïö©\n",
    "#     5) 4-way TTA test inference: [0¬∞,90¬∞] x [flip,no-flip]\n",
    "#     6) Holdout Í∏∞Î∞ò Temp scaling + weight grid search (w>=0.15)\n",
    "#     7) Final ensemble submission\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, re, math, json, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "BASE = \"/data/ephemeral/home/data\"   # ‚úÖ Ïó¨Í∏∞Îßå ÎÑ§ ÌôòÍ≤ΩÏóê ÎßûÍ≤å Ï°∞Ï†ï\n",
    "\n",
    "META_V7      = f\"{BASE}/meta_stage0_7_train_v7.csv\"\n",
    "TRAIN_CSV    = f\"{BASE}/raw/train.csv\"\n",
    "SUB_CSV      = f\"{BASE}/raw/sample_submission.csv\"\n",
    "TRAIN_IMG_V7 = f\"{BASE}/processed/stage0_7_train_v7\"\n",
    "TEST_IMG_V6  = f\"{BASE}/processed/stage0_6_test_v6\"\n",
    "\n",
    "OUT_DIR      = \"./runs_v7_holdout\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUT_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{OUT_DIR}/debug\", exist_ok=True)\n",
    "os.makedirs(f\"{OUT_DIR}/test_probs\", exist_ok=True)\n",
    "\n",
    "CFG = dict(\n",
    "    seed=42,\n",
    "    num_classes=17,\n",
    "    img_size_b3=380,\n",
    "    img_size_vit=384,\n",
    "    img_size_conv=384,\n",
    "    batch_size=16,\n",
    "    epochs=40,\n",
    "    early_stop=12,\n",
    "    lr_b3=1e-4,  wd_b3=1e-4,\n",
    "    lr_vit=5e-5, wd_vit=0.05,\n",
    "    lr_conv=2e-4, wd_conv=0.05,\n",
    "    label_smooth=0.05,\n",
    "    mixup_prob=0.5,\n",
    "    mixup_alpha=0.2,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    tta_angles=[0, 90],\n",
    "    tta_flips=[False, True],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01023126-c73d-43d9-ada0-db39c5582acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Utils\n",
    "# ============================================================\n",
    "def set_seed(seed):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def row_normalize(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    x = np.clip(x, 1e-12, None)\n",
    "    s = x.sum(1, keepdims=True)\n",
    "    s[s == 0] = 1.0\n",
    "    return (x / s).astype(np.float32)\n",
    "\n",
    "def corrcoef(a, b):\n",
    "    a = a.reshape(len(a), -1); b = b.reshape(len(b), -1)\n",
    "    a = a - a.mean(axis=1, keepdims=True)\n",
    "    b = b - b.mean(axis=1, keepdims=True)\n",
    "    an = np.sqrt((a*a).sum(1, keepdims=True)) + 1e-12\n",
    "    bn = np.sqrt((b*b).sum(1, keepdims=True)) + 1e-12\n",
    "    a /= an; b /= bn\n",
    "    return float(np.mean((a*b).sum(1)))\n",
    "\n",
    "def temp_scale(probs, T):\n",
    "    logits = np.log(np.clip(probs, 1e-12, 1.0))\n",
    "    logits /= T\n",
    "    m = logits.max(1, keepdims=True)\n",
    "    e = np.exp(logits - m)\n",
    "    return row_normalize(e)\n",
    "\n",
    "def fit_temperature(probs, targets, t_min=1.0, t_max=2.0, steps=21):\n",
    "    # simple grid-search NLL minimization\n",
    "    ys = np.eye(CFG[\"num_classes\"], dtype=np.float64)[targets]\n",
    "    best_T, best_nll = 1.0, 1e18\n",
    "    for T in np.linspace(t_min, t_max, steps):\n",
    "        p = temp_scale(probs, T)\n",
    "        nll = -np.sum(ys * np.log(np.clip(p,1e-12,1.0))) / len(targets)\n",
    "        if nll < best_nll:\n",
    "            best_nll, best_T = nll, T\n",
    "    return float(best_T)\n",
    "\n",
    "def save_npz(path, arr):\n",
    "    arr = np.asarray(arr, dtype=np.float32)\n",
    "    np.savez_compressed(path, arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9410fdf-ea39-4a7e-aba6-e50913b6fd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Load meta v7 & raw labels\n",
      "‚úÖ Total v7: 7850\n",
      "‚úÖ Train(root) IDs: 1256, Holdout(root) IDs: 314\n",
      "‚úÖ Train samples (incl. aug): 6280\n",
      "‚úÖ Holdout samples (orig only): 314\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Data prep: v7 + holdout split\n",
    "# ============================================================\n",
    "set_seed(CFG[\"seed\"])\n",
    "\n",
    "print(\"üîπ Load meta v7 & raw labels\")\n",
    "meta = pd.read_csv(META_V7)  # filepath, group, basename\n",
    "train_raw = pd.read_csv(TRAIN_CSV)  # ID, target\n",
    "\n",
    "# ID normalizer\n",
    "def norm_id(x):\n",
    "    x = str(x)\n",
    "    return x if x.endswith(\".jpg\") else f\"{x}.jpg\"\n",
    "\n",
    "train_raw[\"ID_norm\"] = train_raw[\"ID\"].apply(norm_id)\n",
    "id2tgt = dict(zip(train_raw[\"ID_norm\"], train_raw[\"target\"]))\n",
    "\n",
    "# root_id Ï∂îÏ∂ú (aug Ï†úÍ±∞)\n",
    "def get_root_id(basename):\n",
    "    b = os.path.splitext(str(basename))[0]\n",
    "    b = b.split(\"_aug\")[0]\n",
    "    return norm_id(b)\n",
    "\n",
    "meta[\"root_id\"] = meta[\"basename\"].apply(get_root_id)\n",
    "meta[\"target\"] = meta[\"root_id\"].map(id2tgt)\n",
    "\n",
    "# Ïú†Ìö®Ìïú Î†àÏΩîÎìúÎßå\n",
    "meta = meta.dropna(subset=[\"target\"]).reset_index(drop=True)\n",
    "meta[\"target\"] = meta[\"target\"].astype(int)\n",
    "\n",
    "# filepath Ïû¨Íµ¨ÏÑ± (metaÏùò filepathÍ∞Ä ÏÉÅÎåÄ/Ï†àÎåÄ ÏÑûÏòÄÏùÑ Ïàò ÏûàÏñ¥ÏÑú Ïã†Î¢∞ Ïïà Ìï®)\n",
    "meta[\"filepath\"] = meta.apply(\n",
    "    lambda r: os.path.join(TRAIN_IMG_V7, str(r[\"group\"]), str(r[\"basename\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ÏõêÎ≥∏(ÎπÑ-aug) Í∏∞Ï§ÄÏúºÎ°ú holdout split: 8:2 stratified\n",
    "orig = meta[~meta[\"basename\"].str.contains(\"_aug\")][[\"root_id\",\"target\"]].drop_duplicates()\n",
    "train_ids, holdout_ids = train_test_split(\n",
    "    orig[\"root_id\"],\n",
    "    test_size=0.2,\n",
    "    random_state=CFG[\"seed\"],\n",
    "    stratify=orig[\"target\"]\n",
    ")\n",
    "train_ids = set(train_ids)\n",
    "holdout_ids = set(holdout_ids)\n",
    "\n",
    "# train: train_idsÏóê ÏÜçÌïú Î™®Îì† (ÏõêÎ≥∏+aug)\n",
    "train_df = meta[meta[\"root_id\"].isin(train_ids)].reset_index(drop=True)\n",
    "\n",
    "# holdout: holdout_idsÏóê ÏÜçÌïú \"ÏõêÎ≥∏Îßå\" (aug Ï†úÏô∏)\n",
    "holdout_df = meta[\n",
    "    (meta[\"root_id\"].isin(holdout_ids)) &\n",
    "    (~meta[\"basename\"].str.contains(\"_aug\"))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Total v7: {len(meta)}\")\n",
    "print(f\"‚úÖ Train(root) IDs: {len(train_ids)}, Holdout(root) IDs: {len(holdout_ids)}\")\n",
    "print(f\"‚úÖ Train samples (incl. aug): {len(train_df)}\")\n",
    "print(f\"‚úÖ Holdout samples (orig only): {len(holdout_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70bd5ff-50a5-4861-9254-68e421b5088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Albumentations transforms\n",
    "# ============================================================\n",
    "def build_transforms(img_size, is_train=True):\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.Rotate(limit=10, border_mode=cv2.BORDER_REFLECT_101, p=0.4),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(0.15, 0.15, p=0.4),\n",
    "            A.GaussianBlur(blur_limit=(3,5), p=0.3),\n",
    "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "# ============================================================\n",
    "# Dataset\n",
    "# ============================================================\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, df, img_size, is_train):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tf = build_transforms(img_size, is_train)\n",
    "        self.is_train = is_train\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = cv2.imread(row[\"filepath\"])\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(row[\"filepath\"])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        aug = self.tf(image=img)[\"image\"]\n",
    "        target = int(row[\"target\"])\n",
    "        return aug, target\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, filepaths, img_size):\n",
    "        self.paths = filepaths\n",
    "        self.tf = build_transforms(img_size, is_train=False)\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.paths[idx])\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(self.paths[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        aug = self.tf(image=img)[\"image\"]\n",
    "        return aug\n",
    "\n",
    "# ============================================================\n",
    "# Model factory\n",
    "# ============================================================\n",
    "def create_model(name, num_classes, drop_rate=0.0, drop_path=0.0):\n",
    "    model = timm.create_model(\n",
    "        name,\n",
    "        pretrained=True,\n",
    "        num_classes=num_classes,\n",
    "        drop_rate=drop_rate,\n",
    "        drop_path_rate=drop_path,\n",
    "        in_chans=3,\n",
    "    )\n",
    "    model.to(CFG[\"device\"])\n",
    "    model.to(memory_format=torch.channels_last)\n",
    "    if hasattr(model, \"set_grad_checkpointing\"):\n",
    "        model.set_grad_checkpointing(True)\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# Train / Validate (holdout)\n",
    "# ============================================================\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(CFG[\"device\"], non_blocking=True)\n",
    "        yb = yb.to(CFG[\"device\"], non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(CFG[\"device\"]==\"cuda\")):\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    probs_all, targs_all = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(CFG[\"device\"], non_blocking=True)\n",
    "        yb = yb.to(CFG[\"device\"], non_blocking=True)\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(CFG[\"device\"]==\"cuda\")):\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            probs = torch.softmax(logits, dim=1).to(torch.float32).cpu().numpy()\n",
    "        total_loss += loss.item()\n",
    "        probs_all.append(probs)\n",
    "        targs_all.append(yb.cpu().numpy())\n",
    "    probs_all = row_normalize(np.vstack(probs_all))\n",
    "    targs_all = np.concatenate(targs_all)\n",
    "    f1 = f1_score(targs_all, probs_all.argmax(1), average=\"macro\")\n",
    "    return total_loss / max(1, len(loader)), f1, probs_all, targs_all\n",
    "\n",
    "def train_backbone(key, train_df, holdout_df):\n",
    "    if key == \"b3\":\n",
    "        name = \"tf_efficientnet_b3_ns\"\n",
    "        img_size = CFG[\"img_size_b3\"]\n",
    "        lr, wd = CFG[\"lr_b3\"], CFG[\"wd_b3\"]\n",
    "        drop_rate, drop_path = 0.4, 0.2\n",
    "    elif key == \"vit\":\n",
    "        name = \"vit_base_patch16_384\"\n",
    "        img_size = CFG[\"img_size_vit\"]\n",
    "        lr, wd = CFG[\"lr_vit\"], CFG[\"wd_vit\"]\n",
    "        drop_rate, drop_path = 0.2, 0.1\n",
    "    elif key == \"conv\":\n",
    "        name = \"convnext_small\"\n",
    "        img_size = CFG[\"img_size_conv\"]\n",
    "        lr, wd = CFG[\"lr_conv\"], CFG[\"wd_conv\"]\n",
    "        drop_rate, drop_path = 0.0, 0.2\n",
    "    else:\n",
    "        raise ValueError(key)\n",
    "\n",
    "    print(f\"\\n==== Train {key} ({name}) ====\")\n",
    "\n",
    "    tr_ds = ImgDataset(train_df, img_size, is_train=True)\n",
    "    ho_ds = ImgDataset(holdout_df, img_size, is_train=False)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=CFG[\"batch_size\"], shuffle=True,\n",
    "                       num_workers=4, pin_memory=True, drop_last=True)\n",
    "    ho_ld = DataLoader(ho_ds, batch_size=CFG[\"batch_size\"], shuffle=False,\n",
    "                       num_workers=4, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = create_model(name, CFG[\"num_classes\"], drop_rate, drop_path)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=CFG[\"label_smooth\"])\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(CFG[\"device\"]==\"cuda\"))\n",
    "\n",
    "    best_f1, best_probs, best_tgts = -1.0, None, None\n",
    "    best_path = f\"{OUT_DIR}/models/{key}_best.pt\"\n",
    "    es = 0\n",
    "\n",
    "    for ep in range(1, CFG[\"epochs\"]+1):\n",
    "        tr_loss = train_one_epoch(model, tr_ld, optimizer, loss_fn, scaler)\n",
    "        va_loss, va_f1, va_probs, va_tgts = validate(model, ho_ld, loss_fn)\n",
    "        print(f\"[{key}] Ep{ep}/{CFG['epochs']} | TrLoss={tr_loss:.4f} \"\n",
    "              f\"VaLoss={va_loss:.4f} VaF1={va_f1:.4f}\")\n",
    "\n",
    "        if va_f1 > best_f1:\n",
    "            best_f1 = va_f1\n",
    "            best_probs, best_tgts = va_probs, va_tgts\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            es = 0\n",
    "        else:\n",
    "            es += 1\n",
    "        if es >= CFG[\"early_stop\"]:\n",
    "            print(f\"[{key}] Early stop at Ep{ep}\")\n",
    "            break\n",
    "\n",
    "    print(f\"‚úÖ {key} best holdout F1 = {best_f1:.4f}\")\n",
    "    del model; torch.cuda.empty_cache(); gc.collect()\n",
    "    return best_path, best_probs, best_tgts, best_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640d59b9-1b8e-4086-b1bc-3a72412404b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TTA inference\n",
    "# ============================================================\n",
    "def rotate_np(img, angle):\n",
    "    if angle == 0: return img\n",
    "    if angle == 90: return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    raise ValueError(\"Only 0 and 90 used here.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_with_tta(key, model_path, filepaths):\n",
    "    if key == \"b3\":\n",
    "        name = \"tf_efficientnet_b3_ns\"\n",
    "        img_size = CFG[\"img_size_b3\"]\n",
    "        drop_rate, drop_path = 0.4, 0.2\n",
    "    elif key == \"vit\":\n",
    "        name = \"vit_base_patch16_384\"\n",
    "        img_size = CFG[\"img_size_vit\"]\n",
    "        drop_rate, drop_path = 0.2, 0.1\n",
    "    elif key == \"conv\":\n",
    "        name = \"convnext_small\"\n",
    "        img_size = CFG[\"img_size_conv\"]\n",
    "        drop_rate, drop_path = 0.0, 0.2\n",
    "    else:\n",
    "        raise ValueError(key)\n",
    "\n",
    "    tf = build_transforms(img_size, is_train=False)\n",
    "\n",
    "    model = create_model(name, CFG[\"num_classes\"], drop_rate, drop_path)\n",
    "    state = torch.load(model_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    probs_total = None\n",
    "    n_tta = len(CFG[\"tta_angles\"]) * len(CFG[\"tta_flips\"])\n",
    "\n",
    "    for ang in CFG[\"tta_angles\"]:\n",
    "        for fl in CFG[\"tta_flips\"]:\n",
    "            batch_tensors = []\n",
    "            for p in filepaths:\n",
    "                img = cv2.imread(p)\n",
    "                if img is None:\n",
    "                    raise FileNotFoundError(p)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = rotate_np(img, ang)\n",
    "                if fl:\n",
    "                    img = cv2.flip(img, 1)\n",
    "                t = tf(image=img)[\"image\"]\n",
    "                batch_tensors.append(t)\n",
    "            ds = torch.stack(batch_tensors)\n",
    "            ld = DataLoader(ds, batch_size=CFG[\"batch_size\"], shuffle=False,\n",
    "                            num_workers=2, pin_memory=True, drop_last=False)\n",
    "            probs_run = []\n",
    "            for xb in ld:\n",
    "                xb = xb.to(CFG[\"device\"], non_blocking=True)\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(CFG[\"device\"]==\"cuda\")):\n",
    "                    logits = model(xb)\n",
    "                    pr = torch.softmax(logits, dim=1).to(torch.float32).cpu().numpy()\n",
    "                probs_run.append(pr)\n",
    "            probs_run = row_normalize(np.vstack(probs_run))\n",
    "            probs_total = probs_run if probs_total is None else (probs_total + probs_run)\n",
    "\n",
    "    del model; torch.cuda.empty_cache(); gc.collect()\n",
    "    return row_normalize(probs_total / n_tta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f9401e-5aec-45bb-a178-93216e9d9a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Train b3 (tf_efficientnet_b3_ns) ====\n",
      "[b3] Ep1/40 | TrLoss=1.2527 VaLoss=0.6379 VaF1=0.8609\n",
      "[b3] Ep2/40 | TrLoss=0.6129 VaLoss=0.5625 VaF1=0.9066\n",
      "[b3] Ep3/40 | TrLoss=0.4942 VaLoss=0.5216 VaF1=0.9364\n",
      "[b3] Ep4/40 | TrLoss=0.4501 VaLoss=0.5185 VaF1=0.9298\n",
      "[b3] Ep5/40 | TrLoss=0.4171 VaLoss=0.4921 VaF1=0.9455\n",
      "[b3] Ep6/40 | TrLoss=0.4004 VaLoss=0.5077 VaF1=0.9392\n",
      "[b3] Ep7/40 | TrLoss=0.3865 VaLoss=0.5232 VaF1=0.9377\n",
      "[b3] Ep8/40 | TrLoss=0.3742 VaLoss=0.5068 VaF1=0.9440\n",
      "[b3] Ep9/40 | TrLoss=0.3705 VaLoss=0.4942 VaF1=0.9389\n",
      "[b3] Ep10/40 | TrLoss=0.3627 VaLoss=0.4908 VaF1=0.9351\n",
      "[b3] Ep11/40 | TrLoss=0.3581 VaLoss=0.4931 VaF1=0.9511\n",
      "[b3] Ep12/40 | TrLoss=0.3550 VaLoss=0.4913 VaF1=0.9416\n",
      "[b3] Ep13/40 | TrLoss=0.3586 VaLoss=0.5278 VaF1=0.9377\n",
      "[b3] Ep14/40 | TrLoss=0.3533 VaLoss=0.5025 VaF1=0.9405\n",
      "[b3] Ep15/40 | TrLoss=0.3478 VaLoss=0.5020 VaF1=0.9337\n",
      "[b3] Ep16/40 | TrLoss=0.3444 VaLoss=0.5016 VaF1=0.9262\n",
      "[b3] Ep17/40 | TrLoss=0.3490 VaLoss=0.4863 VaF1=0.9394\n",
      "[b3] Ep18/40 | TrLoss=0.3490 VaLoss=0.5116 VaF1=0.9428\n",
      "[b3] Ep19/40 | TrLoss=0.3462 VaLoss=0.5174 VaF1=0.9226\n",
      "[b3] Ep20/40 | TrLoss=0.3405 VaLoss=0.4893 VaF1=0.9351\n",
      "[b3] Ep21/40 | TrLoss=0.3377 VaLoss=0.4882 VaF1=0.9476\n",
      "[b3] Ep22/40 | TrLoss=0.3389 VaLoss=0.5553 VaF1=0.9298\n",
      "[b3] Ep23/40 | TrLoss=0.3428 VaLoss=0.5259 VaF1=0.9261\n",
      "[b3] Early stop at Ep23\n",
      "‚úÖ b3 best holdout F1 = 0.9511\n",
      "\n",
      "==== Train vit (vit_base_patch16_384) ====\n",
      "[vit] Ep1/40 | TrLoss=1.1820 VaLoss=0.6226 VaF1=0.8624\n",
      "[vit] Ep2/40 | TrLoss=0.5878 VaLoss=0.5426 VaF1=0.9183\n",
      "[vit] Ep3/40 | TrLoss=0.4730 VaLoss=0.5161 VaF1=0.9082\n",
      "[vit] Ep4/40 | TrLoss=0.4486 VaLoss=0.5600 VaF1=0.9037\n",
      "[vit] Ep5/40 | TrLoss=0.4114 VaLoss=0.5417 VaF1=0.9110\n",
      "[vit] Ep6/40 | TrLoss=0.4000 VaLoss=0.5562 VaF1=0.9166\n",
      "[vit] Ep7/40 | TrLoss=0.3933 VaLoss=0.6547 VaF1=0.9001\n",
      "[vit] Ep8/40 | TrLoss=0.3997 VaLoss=0.6198 VaF1=0.8936\n",
      "[vit] Ep9/40 | TrLoss=0.3792 VaLoss=0.6219 VaF1=0.8943\n",
      "[vit] Ep10/40 | TrLoss=0.3669 VaLoss=0.5673 VaF1=0.9161\n",
      "[vit] Ep11/40 | TrLoss=0.3752 VaLoss=0.5924 VaF1=0.8866\n",
      "[vit] Ep12/40 | TrLoss=0.3870 VaLoss=0.6379 VaF1=0.9003\n",
      "[vit] Ep13/40 | TrLoss=0.3718 VaLoss=0.5746 VaF1=0.9117\n",
      "[vit] Ep14/40 | TrLoss=0.3770 VaLoss=0.5690 VaF1=0.9245\n",
      "[vit] Ep15/40 | TrLoss=0.3650 VaLoss=0.6581 VaF1=0.8931\n",
      "[vit] Ep16/40 | TrLoss=0.3621 VaLoss=0.5992 VaF1=0.8999\n",
      "[vit] Ep17/40 | TrLoss=0.3658 VaLoss=0.5842 VaF1=0.9143\n",
      "[vit] Ep18/40 | TrLoss=0.3680 VaLoss=0.6357 VaF1=0.9031\n",
      "[vit] Ep19/40 | TrLoss=0.3702 VaLoss=0.6050 VaF1=0.9080\n",
      "[vit] Ep20/40 | TrLoss=0.3467 VaLoss=0.7049 VaF1=0.8895\n",
      "[vit] Ep21/40 | TrLoss=0.3591 VaLoss=0.5988 VaF1=0.9204\n",
      "[vit] Ep22/40 | TrLoss=0.3525 VaLoss=0.6310 VaF1=0.9069\n",
      "[vit] Ep23/40 | TrLoss=0.3716 VaLoss=0.6194 VaF1=0.9029\n",
      "[vit] Ep24/40 | TrLoss=0.3512 VaLoss=0.5800 VaF1=0.9029\n",
      "[vit] Ep25/40 | TrLoss=0.3514 VaLoss=0.6939 VaF1=0.8948\n",
      "[vit] Ep26/40 | TrLoss=0.3554 VaLoss=0.6047 VaF1=0.9058\n",
      "[vit] Early stop at Ep26\n",
      "‚úÖ vit best holdout F1 = 0.9245\n",
      "\n",
      "==== Train conv (convnext_small) ====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f45aa7d3bd44c1d83a0d3d3df5252b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[conv] Ep1/40 | TrLoss=2.7335 VaLoss=2.3269 VaF1=0.1022\n",
      "[conv] Ep2/40 | TrLoss=2.1522 VaLoss=1.8299 VaF1=0.3153\n",
      "[conv] Ep3/40 | TrLoss=1.6627 VaLoss=1.2460 VaF1=0.5300\n",
      "[conv] Ep4/40 | TrLoss=1.1912 VaLoss=0.9211 VaF1=0.7424\n",
      "[conv] Ep5/40 | TrLoss=0.9289 VaLoss=0.8435 VaF1=0.7475\n",
      "[conv] Ep6/40 | TrLoss=0.8064 VaLoss=0.7079 VaF1=0.8236\n",
      "[conv] Ep7/40 | TrLoss=0.6974 VaLoss=0.6877 VaF1=0.8377\n",
      "[conv] Ep8/40 | TrLoss=0.6280 VaLoss=0.6226 VaF1=0.8709\n",
      "[conv] Ep9/40 | TrLoss=0.5825 VaLoss=0.6077 VaF1=0.8880\n",
      "[conv] Ep10/40 | TrLoss=0.5270 VaLoss=0.5754 VaF1=0.9128\n",
      "[conv] Ep11/40 | TrLoss=0.5143 VaLoss=0.6255 VaF1=0.8904\n",
      "[conv] Ep12/40 | TrLoss=0.4734 VaLoss=0.5776 VaF1=0.9074\n",
      "[conv] Ep13/40 | TrLoss=0.4686 VaLoss=0.6271 VaF1=0.9041\n",
      "[conv] Ep14/40 | TrLoss=0.4492 VaLoss=0.6188 VaF1=0.8927\n",
      "[conv] Ep15/40 | TrLoss=0.4132 VaLoss=0.6832 VaF1=0.8860\n",
      "[conv] Ep16/40 | TrLoss=0.4002 VaLoss=0.6139 VaF1=0.8961\n",
      "[conv] Ep17/40 | TrLoss=0.4235 VaLoss=0.6421 VaF1=0.8910\n",
      "[conv] Ep18/40 | TrLoss=0.4300 VaLoss=0.5942 VaF1=0.9116\n",
      "[conv] Ep19/40 | TrLoss=0.4047 VaLoss=0.6176 VaF1=0.9022\n",
      "[conv] Ep20/40 | TrLoss=0.3754 VaLoss=0.6244 VaF1=0.9029\n",
      "[conv] Ep21/40 | TrLoss=0.3877 VaLoss=0.6904 VaF1=0.8795\n",
      "[conv] Ep22/40 | TrLoss=0.4217 VaLoss=0.6254 VaF1=0.9077\n",
      "[conv] Early stop at Ep22\n",
      "‚úÖ conv best holdout F1 = 0.9128\n",
      "\n",
      "==== Fit temperature on holdout ====\n",
      "T_b3=1.00, T_vit=1.00, T_conv=1.00\n",
      "\n",
      "==== Search ensemble weights on holdout ====\n",
      "Best Holdout F1=0.9565 | weights: b3=0.70, vit=0.15, conv=0.15\n",
      "\n",
      "==== TTA inference on test (v6) ====\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Weight search (3-model, each w>=0.15)\n",
    "# ============================================================\n",
    "def search_weights_3(p1, p2, p3, y, step=0.05, floor=0.15):\n",
    "    best_f1, best_w = -1.0, (0.6,0.2,0.2)\n",
    "    ws = np.arange(floor, 1.0 + 1e-9, step)\n",
    "    for w1 in ws:\n",
    "        for w2 in ws:\n",
    "            w3 = 1.0 - w1 - w2\n",
    "            if w3 < floor - 1e-9:\n",
    "                continue\n",
    "            mix = row_normalize(w1*p1 + w2*p2 + w3*p3)\n",
    "            f1 = f1_score(y, mix.argmax(1), average=\"macro\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_w = f1, (w1,w2,w3)\n",
    "    return best_w, best_f1\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Train 3 backbones with holdout\n",
    "    b3_path,  b3_ho_p,  ho_y, b3_f1  = train_backbone(\"b3\",  train_df, holdout_df)\n",
    "    vit_path, vit_ho_p, _,   vit_f1 = train_backbone(\"vit\", train_df, holdout_df)\n",
    "    conv_path,conv_ho_p,_,   conv_f1= train_backbone(\"conv\",train_df, holdout_df)\n",
    "\n",
    "    # 2) Temp scaling (per model) using holdout\n",
    "    print(\"\\n==== Fit temperature on holdout ====\")\n",
    "    T_b3  = fit_temperature(b3_ho_p,  ho_y, t_min=1.0, t_max=2.0)\n",
    "    T_vit = fit_temperature(vit_ho_p, ho_y, t_min=1.0, t_max=2.0)\n",
    "    T_conv= fit_temperature(conv_ho_p,ho_y, t_min=1.0, t_max=2.0)\n",
    "    print(f\"T_b3={T_b3:.2f}, T_vit={T_vit:.2f}, T_conv={T_conv:.2f}\")\n",
    "\n",
    "    b3_ho_c   = temp_scale(b3_ho_p,   T_b3)\n",
    "    vit_ho_c  = temp_scale(vit_ho_p,  T_vit)\n",
    "    conv_ho_c = temp_scale(conv_ho_p, T_conv)\n",
    "\n",
    "    # 3) Weight search on holdout (calibrated)\n",
    "    print(\"\\n==== Search ensemble weights on holdout ====\")\n",
    "    (w_b3, w_vit, w_conv), best_ho_f1 = search_weights_3(\n",
    "        b3_ho_c, vit_ho_c, conv_ho_c, ho_y,\n",
    "        step=0.05, floor=0.15\n",
    "    )\n",
    "    print(f\"Best Holdout F1={best_ho_f1:.4f} | weights: \"\n",
    "          f\"b3={w_b3:.2f}, vit={w_vit:.2f}, conv={w_conv:.2f}\")\n",
    "\n",
    "    # 4) Test paths aligned with sample_submission\n",
    "    sub = pd.read_csv(SUB_CSV)\n",
    "    sub_ids = sub[\"ID\"].astype(str).apply(lambda x: x if x.endswith(\".jpg\") else f\"{x}.jpg\")\n",
    "    test_files = []\n",
    "    for name in sub_ids:\n",
    "        cands = []\n",
    "        # allow nested dirs just in case\n",
    "        for root, _, files in os.walk(TEST_IMG_V6):\n",
    "            if name in files:\n",
    "                cands = [os.path.join(root, name)]\n",
    "                break\n",
    "        if cands:\n",
    "            test_files.append(cands[0])\n",
    "        else:\n",
    "            test_files.append(os.path.join(TEST_IMG_V6, name))\n",
    "\n",
    "    print(f\"\\n==== TTA inference on test (v6) ====\")\n",
    "    b3_test   = infer_with_tta(\"b3\",   b3_path,   test_files)\n",
    "    vit_test  = infer_with_tta(\"vit\",  vit_path,  test_files)\n",
    "    conv_test = infer_with_tta(\"conv\", conv_path, test_files)\n",
    "\n",
    "    save_npz(f\"{OUT_DIR}/test_probs/b3_test_probs.npz\",   b3_test)\n",
    "    save_npz(f\"{OUT_DIR}/test_probs/vit_test_probs.npz\",  vit_test)\n",
    "    save_npz(f\"{OUT_DIR}/test_probs/conv_test_probs.npz\", conv_test)\n",
    "\n",
    "    # 5) Apply same temperature scaling to test probs\n",
    "    b3_test_c   = temp_scale(b3_test,   T_b3)\n",
    "    vit_test_c  = temp_scale(vit_test,  T_vit)\n",
    "    conv_test_c = temp_scale(conv_test, T_conv)\n",
    "\n",
    "    # 6) Final ensemble\n",
    "    mix = row_normalize(\n",
    "        w_b3*b3_test_c + w_vit*vit_test_c + w_conv*conv_test_c\n",
    "    )\n",
    "    preds = mix.argmax(1)\n",
    "\n",
    "    # 7) Diagnostics\n",
    "    with open(f\"{OUT_DIR}/debug/ensemble_diagnostics.txt\", \"w\") as f:\n",
    "        f.write(f\"Holdout best F1={best_ho_f1:.4f}\\n\")\n",
    "        f.write(f\"Weights: b3={w_b3:.3f}, vit={w_vit:.3f}, conv={w_conv:.3f}\\n\")\n",
    "        f.write(f\"T: b3={T_b3:.3f}, vit={T_vit:.3f}, conv={T_conv:.3f}\\n\")\n",
    "        f.write(f\"Corr(Eff,ViT)={corrcoef(b3_test, vit_test):.4f}\\n\")\n",
    "        f.write(f\"Corr(Eff,Conv)={corrcoef(b3_test, conv_test):.4f}\\n\")\n",
    "        f.write(f\"Corr(ViT,Conv)={corrcoef(vit_test, conv_test):.4f}\\n\")\n",
    "        f.write(f\"RowSumDelta={np.mean(np.abs(mix.sum(1)-1.0)):.3e}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32106c7-a041-4888-af95-91d08052bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved submission: ./runs_v7_holdout/submission_ensemble_F0.9565_w0.70-0.15-0.15.csv\n"
     ]
    }
   ],
   "source": [
    "    # 8) Save submission\n",
    "    out_name = f\"{OUT_DIR}/submission_ensemble_F{best_ho_f1:.4f}_w{w_b3:.2f}-{w_vit:.2f}-{w_conv:.2f}.csv\"\n",
    "    sub_out = pd.DataFrame({\"ID\": sub_ids, \"target\": preds})\n",
    "    sub_out.to_csv(out_name, index=False)\n",
    "    print(f\"\\n‚úÖ Saved submission: {out_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ae4b7d-789c-4527-9856-2d77a2ac79a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B3_TTA] shape=(3140, 17), rowSumMeanDelta=1.96e-08\n",
      "[VIT_TTA] shape=(3140, 17), rowSumMeanDelta=2.12e-08\n",
      "[CONV_TTA] shape=(3140, 17), rowSumMeanDelta=2.08e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "OUT_DIR = \"./runs_v7_holdout/test_probs\"\n",
    "\n",
    "for key in [\"b3\", \"vit\", \"conv\"]:\n",
    "    path = os.path.join(OUT_DIR, f\"{key}_test_probs.npz\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ö†Ô∏è {key}_test_probs.npz not found\")\n",
    "        continue\n",
    "    arr = np.load(path)[\"arr_0\"]\n",
    "    delta = np.mean(np.abs(arr.sum(1) - 1.0))\n",
    "    print(f\"[{key.upper()}_TTA] shape={arr.shape}, rowSumMeanDelta={delta:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258089e3-45cb-49c7-bbd3-cea9cc378f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
