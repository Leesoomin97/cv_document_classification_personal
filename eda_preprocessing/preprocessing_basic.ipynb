{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9c96ad-cb44-4cd9-818d-2ac407ec2067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SETUP] img_dir = /data/ephemeral/home/data/raw/train\n",
      "[SETUP] save_meta_path = /data/ephemeral/home/data/interim/meta_cleaned.csv\n",
      "ğŸ”§ ì´ 1570ê°œ íŒŒì¼ í›„ë³´ íƒìƒ‰ ì™„ë£Œ\n",
      "ğŸ§¹ ì†ìƒ ì´ë¯¸ì§€ ê²€ì‚¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1570/1570 [00:01<00:00, 935.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ ìœ íš¨ ì´ë¯¸ì§€: 1570\n",
      "â™»ï¸ ì¤‘ë³µ ì´ë¯¸ì§€ ì œê±° ì¤‘ (md5 í•´ì‹œ)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1570/1570 [00:00<00:00, 8586.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ ì¤‘ë³µ ì œê±° í›„: 1570\n",
      "ğŸ“ í•´ìƒë„ í†µê³„ (ìƒ˜í”Œ):\n",
      "            width      height\n",
      "count  100.000000  100.000000\n",
      "mean   495.030000  540.460000\n",
      "std     77.727893   74.905519\n",
      "min    443.000000  401.000000\n",
      "25%    443.000000  443.000000\n",
      "50%    443.000000  591.000000\n",
      "75%    591.000000  591.000000\n",
      "max    653.000000  591.000000\n",
      "ğŸ“ train.csv ê²°í•© ì¤‘...\n",
      "âœ… ë¼ë²¨ ì»¬ëŸ¼ ìë™ ê°ì§€: target\n",
      "ğŸ’¾ meta_cleaned.csv ì €ì¥ ì™„ë£Œ â†’ ../../data/interim/meta_cleaned.csv\n",
      "ğŸªµ ë¡œê·¸ ì €ì¥ ì™„ë£Œ â†’ ../../data/interim/preprocessing_log.txt\n",
      "âœ… Basic Preprocessing ì™„ë£Œ!\n",
      "rows: 1570\n",
      "âœ… meta_cleaned.csv ë¡œë“œ ì™„ë£Œ: (1570, 3)\n",
      "               filepath              filename  target\n",
      "0  002f99746285dfdd.jpg  002f99746285dfdd.jpg      16\n",
      "1  008ccd231e1fea5d.jpg  008ccd231e1fea5d.jpg      10\n",
      "2  008f5911bfda7695.jpg  008f5911bfda7695.jpg      10\n",
      "3  009235e4c9c07af5.jpg  009235e4c9c07af5.jpg       4\n",
      "4  00b2f44967580c74.jpg  00b2f44967580c74.jpg      16\n",
      "âœ… class_name ì»¬ëŸ¼ ë³‘í•© ì™„ë£Œ:                filepath  target                  class_name\n",
      "0  002f99746285dfdd.jpg      16  vehicle_registration_plate\n",
      "1  008ccd231e1fea5d.jpg      10        payment_confirmation\n",
      "2  008f5911bfda7695.jpg      10        payment_confirmation\n",
      "3  009235e4c9c07af5.jpg       4                   diagnosis\n",
      "4  00b2f44967580c74.jpg      16  vehicle_registration_plate\n",
      "âœ… ê·¸ë£¹ ë§¤í•‘ ì™„ë£Œ:\n",
      "               filepath                  class_name         group  group_idx\n",
      "0  002f99746285dfdd.jpg  vehicle_registration_plate       vehicle          3\n",
      "1  008ccd231e1fea5d.jpg        payment_confirmation  medical_cert          0\n",
      "2  008f5911bfda7695.jpg        payment_confirmation  medical_cert          0\n",
      "3  009235e4c9c07af5.jpg                   diagnosis  medical_cert          0\n",
      "4  00b2f44967580c74.jpg  vehicle_registration_plate       vehicle          3\n",
      "ğŸ’¾ meta_grouped.csv ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1ï¸âƒ£ Basic Preprocessing (ìµœì¢…ë³¸)\n",
    "----------------------------------------\n",
    "EDA ì „ì— í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ëŠ” ë°ì´í„° ì •ë¦¬ ë‹¨ê³„ì…ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ“Œ ì£¼ìš” ì‘ì—…\n",
    "1. í™•ì¥ì í†µì¼ (.jpg)\n",
    "2. ì†ìƒ ì´ë¯¸ì§€ ì œê±°\n",
    "3. ì¤‘ë³µ ì´ë¯¸ì§€ ì œê±° (hash ê¸°ë°˜)\n",
    "4. í•´ìƒë„ í†µê³„ ì¶œë ¥\n",
    "5. train.csv ê²°í•© â†’ meta_cleaned.csv ì €ì¥\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, hashlib\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ğŸ§© 1ï¸âƒ£ ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬ í•¨ìˆ˜\n",
    "# ============================================================\n",
    "def is_valid_image(path):\n",
    "    \"\"\"ì´ë¯¸ì§€ íŒŒì¼ì´ ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€ ê²€ì‚¬\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            return False\n",
    "        h, w = img.shape[:2]\n",
    "        return h > 0 and w > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# âš™ï¸ 2ï¸âƒ£ ê¸°ë³¸ ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜\n",
    "# ============================================================\n",
    "def basic_preprocessing(\n",
    "    img_dir=\"../../data/raw/train\",\n",
    "    train_csv=\"../../data/raw/train.csv\",\n",
    "    save_meta_path=\"../../data/interim/meta_cleaned.csv\",\n",
    "    exts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"),\n",
    "    sample_n=100\n",
    "):\n",
    "    print(f\"[SETUP] img_dir = {os.path.abspath(img_dir)}\")\n",
    "    if not os.path.isdir(img_dir):\n",
    "        raise FileNotFoundError(f\"ì´ë¯¸ì§€ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {os.path.abspath(img_dir)}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_meta_path), exist_ok=True)\n",
    "    print(f\"[SETUP] save_meta_path = {os.path.abspath(save_meta_path)}\")\n",
    "\n",
    "    # --- 1. ì´ë¯¸ì§€ íŒŒì¼ íƒìƒ‰ ---\n",
    "    all_paths = []\n",
    "    for ext in exts:\n",
    "        all_paths += glob.glob(os.path.join(img_dir, \"**\", f\"*{ext}\"), recursive=True)\n",
    "        all_paths += glob.glob(os.path.join(img_dir, \"**\", f\"*{ext.upper()}\"), recursive=True)\n",
    "    all_paths = sorted(set(all_paths))\n",
    "    print(f\"ğŸ”§ ì´ {len(all_paths)}ê°œ íŒŒì¼ í›„ë³´ íƒìƒ‰ ì™„ë£Œ\")\n",
    "\n",
    "    # --- 2. ì†ìƒ ì´ë¯¸ì§€ ì œê±° ---\n",
    "    print(\"ğŸ§¹ ì†ìƒ ì´ë¯¸ì§€ ê²€ì‚¬ ì¤‘...\")\n",
    "    valid_paths = [p for p in tqdm(all_paths) if is_valid_image(p)]\n",
    "    print(f\"   â†’ ìœ íš¨ ì´ë¯¸ì§€: {len(valid_paths)}\")\n",
    "\n",
    "    # --- 3. ì¤‘ë³µ ì œê±° (md5 í•´ì‹œ) ---\n",
    "    print(\"â™»ï¸ ì¤‘ë³µ ì´ë¯¸ì§€ ì œê±° ì¤‘ (md5 í•´ì‹œ)...\")\n",
    "    hashes = {}\n",
    "    unique_paths = []\n",
    "    for p in tqdm(valid_paths):\n",
    "        try:\n",
    "            with open(p, \"rb\") as f:\n",
    "                h = hashlib.md5(f.read()).hexdigest()\n",
    "            if h not in hashes:\n",
    "                hashes[h] = p\n",
    "                unique_paths.append(p)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {p} ({e})\")\n",
    "    print(f\"   â†’ ì¤‘ë³µ ì œê±° í›„: {len(unique_paths)}\")\n",
    "\n",
    "    # --- 4. í•´ìƒë„ í†µê³„ ---\n",
    "    print(\"ğŸ“ í•´ìƒë„ í†µê³„ (ìƒ˜í”Œ):\")\n",
    "    sizes = []\n",
    "    for p in unique_paths[:min(sample_n, len(unique_paths))]:\n",
    "        img = cv2.imread(p)\n",
    "        if img is not None:\n",
    "            h, w = img.shape[:2]\n",
    "            sizes.append((w, h))\n",
    "    if sizes:\n",
    "        size_df = pd.DataFrame(sizes, columns=[\"width\", \"height\"])\n",
    "        print(size_df.describe())\n",
    "    else:\n",
    "        print(\"   (í‘œë³¸ì´ ì—†ì–´ í†µê³„ë¥¼ ì¶œë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤)\")\n",
    "\n",
    "    # --- 5. train.csv ê²°í•© ---\n",
    "    print(\"ğŸ“ train.csv ê²°í•© ì¤‘...\")\n",
    "    meta = pd.read_csv(train_csv)\n",
    "\n",
    "    # ID â†’ filename ë§¤ì¹­\n",
    "    if \"ID\" not in meta.columns:\n",
    "        raise KeyError(\"train.csvì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    meta[\"filename\"] = meta[\"ID\"].astype(str)\n",
    "    meta[\"filename\"] = meta[\"filename\"].apply(lambda x: x if x.endswith(\".jpg\") else f\"{x}.jpg\")\n",
    "\n",
    "    # ë¼ë²¨ ì»¬ëŸ¼ ìë™ íƒì§€\n",
    "    label_col = None\n",
    "    for c in meta.columns:\n",
    "        name = c.lower()\n",
    "        if \"label\" in name or \"target\" in name or \"class\" in name:\n",
    "            label_col = c\n",
    "            break\n",
    "    if label_col is None:\n",
    "        raise KeyError(\"train.csvì—ì„œ label/target/class ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(f\"âœ… ë¼ë²¨ ì»¬ëŸ¼ ìë™ ê°ì§€: {label_col}\")\n",
    "\n",
    "    # ê²½ë¡œ ë§¤ì¹­\n",
    "    rel_paths = [os.path.relpath(p, img_dir).replace(\"\\\\\", \"/\") for p in unique_paths]\n",
    "    filenames = [os.path.basename(p) for p in unique_paths]\n",
    "    out_df = pd.DataFrame({\n",
    "        \"filepath\": rel_paths,\n",
    "        \"filename\": filenames\n",
    "    }).merge(meta[[\"filename\", label_col]], on=\"filename\", how=\"left\")\n",
    "\n",
    "    out_df.rename(columns={label_col: \"target\"}, inplace=True)\n",
    "    out_df.to_csv(save_meta_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"ğŸ’¾ meta_cleaned.csv ì €ì¥ ì™„ë£Œ â†’ {save_meta_path}\")\n",
    "\n",
    "    # --- ë¡œê·¸ ì €ì¥ ---\n",
    "    log_path = os.path.join(os.path.dirname(save_meta_path), \"preprocessing_log.txt\")\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Total raw files: {len(all_paths)}\\n\")\n",
    "        f.write(f\"Valid images: {len(valid_paths)}\\n\")\n",
    "        f.write(f\"Unique images: {len(unique_paths)}\\n\")\n",
    "        f.write(f\"Meta entries: {len(meta)}\\n\")\n",
    "        f.write(f\"Cleaned entries: {len(out_df)}\\n\")\n",
    "        f.write(f\"Detected label column: {label_col}\\n\")\n",
    "    print(f\"ğŸªµ ë¡œê·¸ ì €ì¥ ì™„ë£Œ â†’ {log_path}\")\n",
    "\n",
    "    print(\"âœ… Basic Preprocessing ì™„ë£Œ!\")\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ğŸš€ ì‹¤í–‰ ì„¹ì…˜\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    IMG_DIR = \"../../data/raw/train\"\n",
    "    TRAIN_CSV = \"../../data/raw/train.csv\"\n",
    "    SAVE_META = \"../../data/interim/meta_cleaned.csv\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(SAVE_META), exist_ok=True)\n",
    "    cleaned_df = basic_preprocessing(\n",
    "        img_dir=IMG_DIR,\n",
    "        train_csv=TRAIN_CSV,\n",
    "        save_meta_path=SAVE_META\n",
    "    )\n",
    "    print(\"rows:\", len(cleaned_df))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ğŸ§­ 2ï¸âƒ£ ê·¸ë£¹ ë§¤í•‘ ì„¹ì…˜\n",
    "# ============================================================\n",
    "META_PATH = \"../../data/interim/meta_cleaned.csv\"\n",
    "META_MAP = \"../../data/raw/meta.csv\"   # âœ… target â†” class_name ë§¤í•‘í‘œ\n",
    "IMG_DIR = \"../../data/raw/train\"\n",
    "SAVE_DIR = \"../reports/figures\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. ë°ì´í„° ë¡œë“œ ---\n",
    "df = pd.read_csv(META_PATH)\n",
    "print(\"âœ… meta_cleaned.csv ë¡œë“œ ì™„ë£Œ:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# --- 2. class_name ë³‘í•© ---\n",
    "meta_map = pd.read_csv(META_MAP)\n",
    "df = df.merge(meta_map, on=\"target\", how=\"left\")\n",
    "print(\"âœ… class_name ì»¬ëŸ¼ ë³‘í•© ì™„ë£Œ:\", df[[\"filepath\", \"target\", \"class_name\"]].head())\n",
    "\n",
    "# --- 3. í´ë˜ìŠ¤ â†’ ê·¸ë£¹ ë§¤í•‘ ---\n",
    "class_to_group_mapping = {\n",
    "    # ë¹„ì •í˜•\n",
    "    'account_number': 'handwritten',\n",
    "\n",
    "    # ì˜ë£Œê³„ì—´\n",
    "    'application_for_payment_of_pregnancy_medical_expenses': 'medical_cert',\n",
    "    'confirmation_of_admission_and_discharge': 'medical_cert',\n",
    "    'diagnosis': 'medical_cert',\n",
    "    'prescription': 'medical_cert',\n",
    "    'medical_outpatient_certificate': 'medical_cert',\n",
    "    'statement_of_opinion': 'medical_cert',\n",
    "    'payment_confirmation': 'medical_cert',\n",
    "\n",
    "    'medical_bill_receipts': 'medical_receipt',\n",
    "    'pharmaceutical_receipt': 'medical_receipt',\n",
    "\n",
    "    # ì‹ ë¶„ì¦ë¥˜\n",
    "    'driver_lisence': 'identity',  # ì˜¤íƒ€ ê·¸ëŒ€ë¡œ\n",
    "    'national_id_card': 'identity',\n",
    "    'passport': 'identity',\n",
    "\n",
    "    # ìë™ì°¨ ê´€ë ¨\n",
    "    'car_dashboard': 'vehicle',\n",
    "    'vehicle_registration_certificate': 'vehicle',\n",
    "    'vehicle_registration_plate': 'vehicle',\n",
    "\n",
    "    # ì¼ë°˜ë¬¸ì„œ\n",
    "    'resume': 'document',\n",
    "}\n",
    "\n",
    "group_to_idx = {\n",
    "    'medical_cert': 0,\n",
    "    'medical_receipt': 1,\n",
    "    'identity': 2,\n",
    "    'vehicle': 3,\n",
    "    'document': 4,\n",
    "    'handwritten': 5\n",
    "}\n",
    "\n",
    "# --- 4. ë§¤í•‘ ì ìš© ---\n",
    "df[\"group\"] = df[\"class_name\"].map(class_to_group_mapping)\n",
    "df[\"group_idx\"] = df[\"group\"].map(group_to_idx)\n",
    "\n",
    "print(\"âœ… ê·¸ë£¹ ë§¤í•‘ ì™„ë£Œ:\")\n",
    "print(df[[\"filepath\", \"class_name\", \"group\", \"group_idx\"]].head())\n",
    "\n",
    "# --- 5. ê²°ê³¼ ì €ì¥ ---\n",
    "df.to_csv(\"../../data/interim/meta_grouped.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ğŸ’¾ meta_grouped.csv ì €ì¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b8e51-0146-4d15-8da5-f59e1fd1b685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
